{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7d4jQRxve37"
   },
   "source": [
    "# [Part 0] Energy Based Models\n",
    "\n",
    "How can we estimate a probability distribution $p(\\mathbf{x})$ across such a vast number of dimensions (for instance, the MNIST dataset images) using a basic neural network? The challenge lies in the fact that we cannot merely forecast a score between 0 and 1. A valid probability distribution over data must satisfy two fundamental properties:\n",
    "\n",
    "1. The probability distribution must assign a non-negative value to every possible $\\mathbf{x}$: $p(\\mathbf{x}) \\geq 0$.\n",
    "\n",
    "2. The probability density must sum or integrate to 1 across the entire range of potential inputs: $\\int_{\\mathbf{x}} p(\\mathbf{x}) d\\mathbf{x} = 1$.\n",
    "\n",
    "Fortunately, several methods exist to achieve this, one class of which are energy-based models. The core concept behind energy-based models is that any function that yields values greater than zero can be transformed into a probability distribution by normalizing it with respect to its volume. Consider a neural network with a single neuron in its output, akin to regression. We can denote this network as $f_{\\theta}(\\mathbf{x})$, with $\\theta$ representing the network's parameters and $\\mathbf{x}$ denoting the input data, such as an image. The output of $f_{\\theta}$ is a scalar value ranging from $-\\infty$ to $\\infty$. Now, we can use basic probability theory to *normalize* the scores of all possible inputs:\n",
    "\n",
    "$$\n",
    "q_{\\theta}(\\mathbf{x}) = \\frac{\\exp\\left(f_{\\theta}(\\mathbf{x})\\right)}{Z_{\\theta}} \\hspace{5mm}\\text{where}\\hspace{5mm}\n",
    "Z_{\\theta} = \\begin{cases}\n",
    "    \\int_{\\mathbf{x}}\\exp\\left(f_{\\theta}(\\mathbf{x})\\right) d\\mathbf{x} & \\text{if }x\\text{ is continuous}\\\\\n",
    "    \\sum_{\\mathbf{x}}\\exp\\left(f_{\\theta}(\\mathbf{x})\\right) & \\text{if }x\\text{ is discrete}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The exponential function serves to guarantee that we attribute a probability greater than zero to all possible inputs. In some instances within the literature, an alternative notation is used: $f_{\\theta}(\\mathbf{x})$ is defined as $-E_{\\theta}(\\mathbf{x})$, with $E_{\\theta}$ representing the energy function. In this convention, data points with a high likelihood are associated with lower energy values, while data points with a low likelihood are linked to higher energy values. The term $Z_{\\theta}$ is our normalization factor, ensuring that the density integrates or sums up to 1. This can be demonstrated by performing an integral over $q_{\\theta}(\\mathbf{x})$:\n",
    "\n",
    "$$\n",
    "\\int_{\\mathbf{x}}q_{\\theta}(\\mathbf{x})d\\mathbf{x} =\n",
    "\\int_{\\mathbf{x}}\\frac{\\exp\\left(f_{\\theta}(\\mathbf{x})\\right)}{\\int_{\\mathbf{\\tilde{x}}}\\exp\\left(f_{\\theta}(\\mathbf{\\tilde{x}})\\right) d\\mathbf{\\tilde{x}}}d\\mathbf{x} =\n",
    "\\frac{\\int_{\\mathbf{x}}\\exp\\left(f_{\\theta}(\\mathbf{x})\\right)d\\mathbf{x}}{\\int_{\\mathbf{\\tilde{x}}}\\exp\\left(f_{\\theta}(\\mathbf{\\tilde{x}})\\right) d\\mathbf{\\tilde{x}}} = 1\n",
    "$$\n",
    "\n",
    "The significant advantage of this probability distribution formulation lies in its adaptability, allowing us to define $f_{\\theta}$ in any way we choose, free from constraints. Nevertheless, when we examine the equation above, a fundamental problem arises: How can we compute $Z_{\\theta}$? It's practically impossible to analytically determine $Z_{\\theta}$ when dealing with high-dimensional inputs and/or larger neural networks. However, it is imperative to have knowledge of $Z_{\\theta}$ for the task at hand. This is where the concept of Contrastive Divergence comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Divergence\n",
    "\n",
    "Generative models are typically trained using maximum likelihood estimation, where the goal is to maximize the likelihood of the instances within the training dataset. Since the precise likelihood of a data point can't be determined due to the unknown normalization constant  $Z_{\\theta}$, training energy-based models requires a slightly different approach. It's not sufficient to merely maximize the unnormalized probability $\\exp(f_{\\theta}(\\mathbf{x}_{\\text{train}}))$ because there's no assurance that $Z{\\theta}$ will remain constant or that $\\mathbf{x}{\\text{train}}$ will become more likely than other data points. However, by framing our training around a comparison of point likelihoods, we can establish a stable training objective. In other words, we can rephrase our maximum likelihood objective to focus on maximizing the probability of $\\mathbf{x}_{\\text{train}}$ in relation to a randomly selected data point from our model:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\nabla_{\\theta}\\mathcal{L}_{\\text{MLE}}(\\mathbf{\\theta};p) & = -\\mathbb{E}_{p(\\mathbf{x})}\\left[\\nabla_{\\theta}\\log q_{\\theta}(\\mathbf{x})\\right]\\\\[5pt]\n",
    "    & = \\mathbb{E}_{p(\\mathbf{x})}\\left[\\nabla_{\\theta}f_{\\theta}(\\mathbf{x})\\right] - \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[\\nabla_{\\theta}f_{\\theta}(\\mathbf{x})\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Given that the loss function is still the objective we want to optimize for, we seek to minimize the energy for data points obtained from the dataset while simultaneously maximizing the energy for data points randomly selected from our model (the sampling process will be elucidated later). While this objective may seem intuitively sound, the question arises: how is it derived from our initial distribution $q_{\\theta}(\\mathbf{x})$? The key lies in our approximation of $Z_{\\theta}$ through a single Monte-Carlo sample. This approximation aligns precisely with the objective previously described.\n",
    "\n",
    "We can get a better idea of how this objective works by revisiting the image we saw in class:\n",
    "\n",
    "<center width=\"100%\"><img src=\"imgs/contrastive_divergence.svg\" width=\"700px\"></center>\n",
    "\n",
    "The point on the right, called \"correct answer\", represents a data point from the dataset (i.e. $x_{\\text{train}}$), and the left point, \"wrong answer\", a sample from our model (i.e. $x_{\\text{sample}}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7d4jQRxve37"
   },
   "source": [
    "# [Part 1] Import Libraries and Load Data\n",
    "\n",
    "First, let's import our standard libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS-g4evave38"
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial8\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDedSf_Ove38"
   },
   "source": [
    "We also have pre-trained models that we download below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCmh2VQGve38"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial8/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"MNIST.ckpt\", \"tensorboards/events.out.tfevents.MNIST\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_Nz6h9Zve3_"
   },
   "source": [
    "## Image generation\n",
    "\n",
    "As an example for energy-based models, we will train a model on image generation. Specifically, we will look at how we can generate MNIST digits with a very simple CNN model. However, it should be noted that energy models are not easy to train and often diverge if the hyperparameters are not well tuned. We will rely on training tricks proposed in the paper [Implicit Generation and Generalization in Energy-Based Models](https://arxiv.org/abs/1903.08689) by Yilun Du and Igor Mordatch ([blog](https://openai.com/blog/energy-based-models/)). The important part of this notebook is however to see how the theory above can actually be used in a model.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "First, we can load the MNIST dataset below. Note that we need to normalize the images between -1 and 1 instead of mean 0 and std 1 because during sampling, we have to limit the input space. Scaling between -1 and 1 makes it easier to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J3p_ioDve4A"
   },
   "outputs": [],
   "source": [
    "# Transformations applied on each image => make them a tensor and normalize between -1 and 1\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))\n",
    "                               ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_set = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True,  drop_last=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = data.DataLoader(test_set,  batch_size=256, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfX6Fbm7ve4A"
   },
   "source": [
    "# [Part 2] Model construction and training\n",
    "\n",
    "### CNN Model\n",
    "\n",
    "We will start by implementing our CNN model. Our model will apply several convolutions with stride 2 that downscale the images. If you are interested, you can also use a deeper model such as a small ResNet, but for simplicity, we suggest sticking with the tiny network.\n",
    "\n",
    "The architecture is as follows:<pre>\n",
    "    01. <b>Conv2D</b> layer          with <font color='blue'><b>16</b></font> filters of <font color='blue'><b>5x5</b></font> using stride <font color='blue'><b>2</b></font> and padding <font color='blue'><b>4</b></font>,\n",
    "    02. <b>Swish</b> activation  \n",
    "    03. <b>Conv2D</b> layer          with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font> using stride <font color='blue'><b>2</b></font> and padding <font color='blue'><b>1</b></font>,\n",
    "    04. <b>Swish</b> activation  \n",
    "    05. <b>Conv2D</b> layer          with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>3x3</b></font> using stride <font color='blue'><b>2</b></font> and padding <font color='blue'><b>1</b></font>,\n",
    "    06. <b>Swish</b> activation  \n",
    "    07. <b>Conv2D</b> layer          with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>3x3</b></font> using stride <font color='blue'><b>2</b></font> and padding <font color='blue'><b>1</b></font>,\n",
    "    08. <b>Swish</b> activation  \n",
    "    09. <b>Flatten</b> layer         to reshape activation into vector\n",
    "    10. <b>Fully Connected</b>       with input size <font color='blue'><b>256</b></font> and output size <font color='blue'><b>64</b></font>,\n",
    "    11. <b>Swish</b> activation  \n",
    "    12. <b>Fully Connected</b>       with input size <font color='blue'><b>64</b></font> and output size <font color='blue'><b>1</b></font>,\n",
    "</pre>\n",
    "\n",
    "Note that the activation function will be Swish instead of the standard ReLU in our energy model. \n",
    "\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HhLHGqC3ve4B"
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE ##########\n",
    "        # Series of convolutions and Swish activation functions\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            \n",
    "                # add Conv2d() from .nn module with 16 Filters, kernel size 5, Stride 2 and padding 4\n",
    "                # (from in_channels = 1)\n",
    "                ??, \n",
    "            \n",
    "                # add Swish() activation\n",
    "                ??,\n",
    "            \n",
    "                # add Conv2d() from .nn module with 32 Filters, kernel size 3, Stride 2 and padding 1\n",
    "                # (from in_channels = 16)\n",
    "                ??, \n",
    "            \n",
    "                # add Swish() activation\n",
    "                ??,\n",
    "                \n",
    "                # add the remaining layers as described in the cell above\n",
    "            \n",
    "                ??,\n",
    "            \n",
    "                ??,\n",
    "            \n",
    "                ??, \n",
    "            \n",
    "                ??,\n",
    "            \n",
    "                ??,\n",
    "            \n",
    "                ??,\n",
    "            \n",
    "                ??,\n",
    "            \n",
    "                ??\n",
    "        )\n",
    "        ####################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1x4rhs1Rve4B"
   },
   "source": [
    "In the rest of the notebook, the output of the model will represent $f_{\\theta}(\\mathbf{x})$ (or equivalently, $-E_{\\theta}(\\mathbf{x})$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Energy-Based Models\n",
    "\n",
    "A commonly used method for generating samples from an energy-based model involves using Markov Chain Monte Carlo with Langevin Dynamics. The basic premise of this algorithm is to start the process from a random point and gradually move in the direction of higher probability by using the gradients of $f_{\\theta}$. Nonetheless, this approach alone is insufficient for fully capturing the probability distribution. To achieve this, we must introduce some noise, denoted as $\\omega$, at each gradient step while moving through the current sample. Performing an infinite number of gradient steps would allow us to generate an exact sample from our modeled distribution. However, since this is not practically feasible, we often constrain the chain to a finite number of steps, denoted as $K$ (a hyperparameter that requires fine-tuning). The overall sampling process can be summarized through the following algorithm:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:15px\"><img src=\"imgs/sampling.svg\" width=\"750px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-pnu3LYve4B"
   },
   "source": [
    "### Sampling buffer\n",
    "\n",
    "In the following section, we delve into the training process with sampled elements. To use the contrastive divergence objective, we must generate samples during training. Previous research has demonstrated that, due to the high dimensionality of images, a considerable number of iterations within the Markov Chain Monte Carlo (MCMC) sampling is necessary to obtain reasonable samples. However, there's a training technique that significantly reduces the cost of sampling: the use of a sampling buffer. The concept behind this is to retain the samples from the most recent few batches in a buffer and utilize them as the starting point for the MCMC algorithm in subsequent batches. This reduction in the sampling cost is attributed to the model requiring fewer steps to converge to acceptable samples. To ensure that we don't solely rely on previous samples and allow for the introduction of new samples, we initialize 5% of our samples from scratch, involving random noise values ranging between -1 and 1.\n",
    "\n",
    "Here, we implement the sampling buffer. The function `sample_new_exmps` generates a fresh batch of \"fake\" images, so named because they have been generated but are not actually part of the dataset. As mentioned previously, 5% of these are initialized randomly, while the remaining 95% are selected at random from our buffer. In this initial batch, we conduct MCMC for 60 iterations to enhance image quality and bring the samples closer to those derived from $q_{\\theta}(\\mathbf{x})$. In the `generate_samples` function, you will implement the MCMC process for images. The hyperparameters such as `step_size`, `steps`, and the noise standard deviation $\\sigma$ have been specifically configured for the MNIST dataset.\n",
    "\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1j_-m60Uve4B"
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for getting a new batch of \"fake\" images.\n",
    "        Inputs:\n",
    "            steps - Number of iterations in the MCMC algorithm\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
    "        \"\"\"\n",
    "        Function for sampling images for a given model.\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
    "            steps - Number of iterations in the MCMC algorithm.\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
    "        \"\"\"\n",
    "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "        # because we are only interested in the gradients of the input.\n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "\n",
    "        # Enable gradient calculation if not already the case\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "        # More efficient than creating a new tensor every iteration.\n",
    "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
    "\n",
    "        # List for storing generations at each step (for later analysis)\n",
    "        imgs_per_step = []\n",
    "\n",
    "        ######### YOUR CODE HERE ##########\n",
    "        # Langevin MCMC loop \n",
    "        for _ in range(steps):\n",
    "            # Part 1: Add noise to the input.\n",
    "            # use the normal_() method on the noise buffer to sample noise with mean=0 and std =0.005\n",
    "            ??\n",
    "            # use the add_() method on inp_imgs.data to add the newly sampled noise.data\n",
    "            ??\n",
    "            # use the clamp_() method on inp_imgs.data to clip the results to the interval [-1,1]\n",
    "            ??\n",
    "\n",
    "            # Part 2: Calculate gradients for the current input.\n",
    "            out_imgs = -model(inp_imgs)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "            # Part 3: Apply gradients to our current samples\n",
    "            # use the add_() method on inp_imgs.data to add the calculated gradient (inp_imgs.grad.data)\n",
    "            # multiplied by -step_size (note the negative sign since we are moving in the \n",
    "            # negative gradient direction)\n",
    "            ??\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            # use the clamp_() method on inp_imgs.data to clip the results to the interval [-1,1]\n",
    "            ??\n",
    "\n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone().detach())\n",
    "        ####################################\n",
    "                \n",
    "        # Reactivate gradients for parameters for training\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "\n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3pGED9Xve4C"
   },
   "source": [
    "### Training algorithm\n",
    "\n",
    "With the sampling buffer ready, we can complete our training algorithm. Below is a summary of the full training algorithm of an energy model for image modeling:\n",
    "\n",
    "<center width=\"100%\" style=\"padding: 15px\"><img src=\"imgs/training_algorithm.svg\" width=\"700px\"></center>\n",
    "\n",
    "In each training iteration, the initial steps correspond to the acquisition of real and synthetic data, as we've discussed regarding the sample buffer. We then compute the contrastive divergence objective using our energy model $f_{\\theta}(x)=-E_{\\theta}(x)$. However, there's an additional training technique we will use, which involves introducing a regularization loss on the output of $f_{\\theta}$. Given that the regularization loss is of lesser importance compared to the contrastive divergence, we introduce a weight factor $\\alpha$, which is typically considerably smaller than 1. *You need to implement both the contrastive divergence as well as the regularization loss, and combine them into the full loss term.* Finally, we execute an update step using an optimizer on the combined loss and add the newly generated samples to the buffer.\n",
    "\n",
    "Below, we put this training dynamic into a PyTorch Lightning module. \n",
    "\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sOGctW-dve4D"
   },
   "outputs": [],
   "source": [
    "class DeepEnergyModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, img_shape, batch_size, alpha=0.1, lr=1e-4, beta1=0.0, **CNN_args):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.cnn = CNNModel(**CNN_args)\n",
    "        self.sampler = Sampler(self.cnn, img_shape=img_shape, sample_size=batch_size)\n",
    "        self.example_input_array = torch.zeros(1, *img_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Energy models can have issues with momentum as the loss surfaces changes with its parameters.\n",
    "        # Hence, we set it to 0 by default.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, 0.999))\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97) # Exponential decay over epochs\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # We add minimal noise to the original images to prevent the model from focusing on purely \"clean\" inputs\n",
    "        real_imgs, _ = batch\n",
    "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "        # Obtain samples\n",
    "        fake_imgs = self.sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "\n",
    "        # Predict energy score for all images\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
    "\n",
    "        ######### YOUR CODE HERE ##########\n",
    "        # Calculate losses\n",
    "        # calculate the regularization loss as the mean of the sum of squares of real_out and fake_out\n",
    "        # (remember you can use the built-in method mean() in any tensor, i.e. (a+b).mean() would give \n",
    "        # the mean of the sum of tensors a and b)\n",
    "        reg_loss = ??\n",
    "        # calculate the contrastive divergence loss as the difference between the means of fake_out and real_out\n",
    "        cdiv_loss = ??\n",
    "        # calculate the full loss as cdiv + alpha*reg (note alpha is stored in self.hparams.alpha)\n",
    "        loss = ??\n",
    "        ####################################\n",
    "\n",
    "        # Logging\n",
    "        self.log('loss', loss)\n",
    "        self.log('loss_regularization', reg_loss)\n",
    "        self.log('loss_contrastive_divergence', cdiv_loss)\n",
    "        self.log('metrics_avg_real', real_out.mean())\n",
    "        self.log('metrics_avg_fake', fake_out.mean())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # For validating, we calculate the contrastive divergence between purely random images and unseen examples\n",
    "        # Note that the validation/test step of energy-based models depends on what we are interested in the model\n",
    "        real_imgs, _ = batch\n",
    "        fake_imgs = torch.rand_like(real_imgs) * 2 - 1\n",
    "\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
    "\n",
    "        cdiv = fake_out.mean() - real_out.mean()\n",
    "        self.log('val_contrastive_divergence', cdiv)\n",
    "        self.log('val_fake_out', fake_out.mean())\n",
    "        self.log('val_real_out', real_out.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-XsQRH-ve4D"
   },
   "source": [
    "We do not include a formal test step because energy-based generative models typically do not undergo evaluation with a dedicated test set. Nevertheless, the validation step serves the purpose of providing insights into the difference between the energy or likelihood of random images and previously unseen examples from the dataset. Alternative testing methods could involve the generation of new images and evaluating their feasibility using metrics like FID or Inception score, or attempting to remove noise from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5IpfxFdve4D"
   },
   "source": [
    "### Callbacks\n",
    "\n",
    "To monitor our model's performance throughout the training process, we'll extensively utilize PyTorch Lightning's callback framework. It's worth noting that callbacks enable the execution of small functions at various points during training, often after completing an epoch. In this context, we have developed three distinct callbacks ourselves.\n",
    "\n",
    "If you want to see the progress of the loss terms as well as the generative quality of the model across the training process you can launch tensorboard and look at the stored logs after training. The initial callback, called `GenerateCallback`, is used to incorporate image generations into the model's training process. After every $N$ epochs (typically, $N=5$ to reduce the output to TensorBoard), we select a small batch of random images and conduct numerous MCMC iterations until the model's image generation reaches a stable state. In contrast to the training, where 60 iterations were used, we perform 256 iterations here. This higher number is chosen because (1) we only need to perform it once as opposed to the training, which does it every iteration, and (2) we initiate the process from scratch rather than using a buffer. The implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "leaNpGwcve4E"
   },
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, batch_size=8, vis_steps=8, num_steps=256, every_n_epochs=5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size         # Number of images to generate\n",
    "        self.vis_steps = vis_steps           # Number of steps within generation to visualize\n",
    "        self.num_steps = num_steps           # Number of steps to take during generation\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        # Skip for all other epochs\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Generate images\n",
    "            imgs_per_step = self.generate_imgs(pl_module)\n",
    "            # Plot and add to tensorboard\n",
    "            for i in range(imgs_per_step.shape[1]):\n",
    "                step_size = self.num_steps // self.vis_steps\n",
    "                imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
    "                grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1))\n",
    "                trainer.logger.experiment.add_image(f\"generation_{i}\", grid, global_step=trainer.current_epoch)\n",
    "\n",
    "    def generate_imgs(self, pl_module):\n",
    "        pl_module.eval()\n",
    "        start_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
    "        start_imgs = start_imgs * 2 - 1\n",
    "        torch.set_grad_enabled(True)  # Tracking gradients for sampling necessary\n",
    "        imgs_per_step = Sampler.generate_samples(pl_module.cnn, start_imgs, steps=self.num_steps, step_size=10, return_img_per_step=True)\n",
    "        torch.set_grad_enabled(False)\n",
    "        pl_module.train()\n",
    "        return imgs_per_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuOMHZONve4E"
   },
   "source": [
    "The next callback, named `SamplerCallback`, essentially involves selecting a randomly chosen subset of images from the sampling buffer and displaying them on TensorBoard. This process aids in gaining insight into the specific images being presented to the model as \"fake\" during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-RsH1jDeve4E"
   },
   "outputs": [],
   "source": [
    "class SamplerCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, num_imgs=32, every_n_epochs=5):\n",
    "        super().__init__()\n",
    "        self.num_imgs = num_imgs             # Number of images to plot\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            exmp_imgs = torch.cat(random.choices(pl_module.sampler.examples, k=self.num_imgs), dim=0)\n",
    "            grid = torchvision.utils.make_grid(exmp_imgs, nrow=4, normalize=True, range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"sampler\", grid, global_step=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_qzKSkpve4E"
   },
   "source": [
    "Lastly, we have the `OutlierCallback`. This callback assesses the model's performance by logging the negative energy values assigned to random noise. Unlike our training loss, which remains relatively stable throughout iterations, this score likely indicates the model's progress in identifying \"outliers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Uf1LQqg_ve4G"
   },
   "outputs": [],
   "source": [
    "class OutlierCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, batch_size=1024):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        with torch.no_grad():\n",
    "            pl_module.eval()\n",
    "            rand_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
    "            rand_imgs = rand_imgs * 2 - 1.0\n",
    "            rand_out = pl_module.cnn(rand_imgs).mean()\n",
    "            pl_module.train()\n",
    "\n",
    "        trainer.logger.experiment.add_scalar(\"rand_out\", rand_out, global_step=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpWdfStQve4G"
   },
   "source": [
    "### Running the model\n",
    "\n",
    "We can now assemble all the components to construct our training function. This function closely resembles other PyTorch Lightning training functions we've encountered. Nevertheless, there is a slight deviation in that we do not conduct testing on a dedicated test set. The model's evaluation will be carried out afterward by assessing its predictions and its capacity for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j7bMrF7gve4G"
   },
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"MNIST\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=60,\n",
    "                         gradient_clip_val=0.1,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor='val_contrastive_divergence'),\n",
    "                                    GenerateCallback(every_n_epochs=5),\n",
    "                                    SamplerCallback(every_n_epochs=5),\n",
    "                                    OutlierCallback(),\n",
    "                                    LearningRateMonitor(\"epoch\")\n",
    "                                   ])\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"MNIST.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = DeepEnergyModel.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = DeepEnergyModel(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = DeepEnergyModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # No testing as we are more interested in other properties\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MlQsWvzve4G",
    "outputId": "2d983fc5-1756-4c2c-f9fa-4f331285ca6e"
   },
   "outputs": [],
   "source": [
    "model = train_model(img_shape=(1,28,28),\n",
    "                    batch_size=train_loader.batch_size,\n",
    "                    lr=1e-4,\n",
    "                    beta1=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUK0Unnqve4H"
   },
   "source": [
    "# [Part 3] Analysis\n",
    "\n",
    "\n",
    "In this last part of this notebook, we will try to analyze the properties of our trained generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_C8tL4Eve4J"
   },
   "source": [
    "### Image Generation\n",
    "\n",
    "A standard approach for assessing generative models involves generating a handful of sample images. The proficiency of these models is truly demonstrated when they can effectively produce realistic images, showcasing their ability to model the actual data distribution. Therefore, let's generate a few images from the model and display them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qw_Wyuave4J"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "pl.seed_everything(43)\n",
    "callback = GenerateCallback(batch_size=4, vis_steps=8, num_steps=256)\n",
    "imgs_per_step = callback.generate_imgs(model)\n",
    "imgs_per_step = imgs_per_step.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6fo2KUcve4J"
   },
   "source": [
    "We know that sampling from energy-based models requires iteratively using the MCMC algorithm. To gain more insight into how this process affects the images, let's plot a few intermediate MCMC samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ivGusjvve4J",
    "outputId": "8ed09f78-4bf1-4d96-d14f-c123ecc63e7d"
   },
   "outputs": [],
   "source": [
    "for i in range(imgs_per_step.shape[1]):\n",
    "    step_size = callback.num_steps // callback.vis_steps\n",
    "    imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
    "    imgs_to_plot = torch.cat([imgs_per_step[0:1,i],imgs_to_plot], dim=0)\n",
    "    grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(grid)\n",
    "    plt.xlabel(\"Generation iteration\")\n",
    "    plt.xticks([(imgs_per_step.shape[-1]+2)*(0.5+j) for j in range(callback.vis_steps+1)],\n",
    "               labels=[1] + list(range(step_size,imgs_per_step.shape[0]+1,step_size)))\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K964GYWRve4K"
   },
   "source": [
    "### Out-of-distribution detection\n",
    "\n",
    "A widely used and robust application of energy-based models is the task of out-of-distribution detection, sometimes referred to as \"anomaly\" detection. In an era where deep learning models find increasing utility in production and various applications, it becomes imperative to understand the boundaries of what these models are unfamiliar with. Deep learning models, by nature, tend to be overly confident, often assigning high probabilities to even random images, which is clearly undesirable in practical applications. Energy-based models offer a solution to this challenge because they are specifically trained to identify images that deviate from the distribution of the training dataset. In practical scenarios, you could train an energy-based model alongside a classifier and only rely on the classifier's predictions if the energy-based model assigns an (unnormalized) probability greater than a certain threshold, denoted as $\\delta$, to the image. In fact, you can integrate classifier and energy-based objectives into a single model, as proposed in this [paper](https://arxiv.org/abs/1912.03263).\n",
    "\n",
    "In this section, we will assess our energy-based model's ability to detect out-of-distribution data. It's important to bear in mind that a lower model output corresponds to a lower probability. Therefore, we anticipate observing low scores when we input random noise into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJKJjkXkve4K",
    "outputId": "0aa6eb56-4b46-495c-ed96-4ee8a1fa26a2"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rand_imgs = torch.rand((128,) + model.hparams.img_shape).to(model.device)\n",
    "    rand_imgs = rand_imgs * 2 - 1.0\n",
    "    rand_out = model.cnn(rand_imgs).mean()\n",
    "    print(f\"Average score for random images: {rand_out.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRhgRK6wve4K",
    "outputId": "04142854-efa6-4972-aed1-430ffebd5c64"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_imgs,_ = next(iter(train_loader))\n",
    "    train_imgs = train_imgs.to(model.device)\n",
    "    train_out = model.cnn(train_imgs).mean()\n",
    "    print(f\"Average score for training images: {train_out.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ki0TDK1ve4L"
   },
   "source": [
    "What happens if we change the training images a little, and see which ones gets a very low score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LYLniGu5ve4L"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compare_images(img1, img2):\n",
    "    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n",
    "    score1, score2 = model.cnn(imgs).cpu().chunk(2, dim=0)\n",
    "    grid = torchvision.utils.make_grid([img1.cpu(), img2.cpu()], nrow=2, normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(grid)\n",
    "    plt.xticks([(img1.shape[2]+2)*(0.5+j) for j in range(2)],\n",
    "               labels=[\"Original image\", \"Transformed image\"])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    print(f\"Score original image: {score1.item():4.2f}\")\n",
    "    print(f\"Score transformed image: {score2.item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcGX5JKRve4L"
   },
   "source": [
    "We use a random test image for this. Feel free to change it to experiment with the model yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IiXYZpgQve4L"
   },
   "outputs": [],
   "source": [
    "test_imgs, _ = next(iter(test_loader))\n",
    "exmp_img = test_imgs[0].to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDxiaua_ve4M"
   },
   "source": [
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "The first transformation is to add some random noise to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO9m4aY-ve4M",
    "outputId": "29ef6308-29aa-48c8-e145-a265457606cf"
   },
   "outputs": [],
   "source": [
    "# add gaussian noise (0 mean, 0.3 variancve) to exmp_img by using torch.randn_like()\n",
    "# note you can modify the variance by multiplying the noise term by the desired variance\n",
    "img_noisy = ??\n",
    "# clamp the resulting noisy image so the values are within the range [-1,1]\n",
    "??\n",
    "\n",
    "compare_images(exmp_img, img_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk5aaTOGve4N"
   },
   "source": [
    "\n",
    "\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "Next, we flip an image and check how this influences the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7KGuyfrve4N",
    "outputId": "ec8ddc80-d4eb-4694-ed47-e85888c90685"
   },
   "outputs": [],
   "source": [
    "# flip exmp_image along both dimensions using the flip() method\n",
    "img_flipped = ??\n",
    "compare_images(exmp_img, img_flipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_8dSIUAve4N"
   },
   "source": [
    "Finally, we check what happens if we reduce the digit significantly in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CroAcinMve4N",
    "outputId": "b070ebc5-6d30-4af9-afce-c4bf320a596f"
   },
   "outputs": [],
   "source": [
    "img_tiny = torch.zeros_like(exmp_img)-1\n",
    "img_tiny[:,exmp_img.shape[1]//2:,exmp_img.shape[2]//2:] = exmp_img[:,::2,::2]\n",
    "compare_images(exmp_img, img_tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07IHCN1tve4N"
   },
   "source": [
    "Do you think the model is good at detecting differences between original and transformed images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:hw3-p2]",
   "language": "python",
   "name": "conda-env-hw3-p2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
