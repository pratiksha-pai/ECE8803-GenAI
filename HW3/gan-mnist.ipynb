{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYvvSQVSctSv"
   },
   "source": [
    "\n",
    "## 1 - GAN Basics\n",
    "\n",
    "We have seen in class that when designing a GAN, we build two different neural networks.\n",
    "\n",
    "The first one is a traditional classification network, called the **DISCRIMINATOR**. The discriminator takes images, and classifies them as being real (belonging to the training set) or fake (not present in the training set).\n",
    "\n",
    "Our other network, called the **GENERATOR**, will take random noise as input and transform it using a neural network to produce images. The goal of the generator is to fool the discriminator into thinking the images it produced are real.\n",
    "\n",
    "<br>\n",
    "\n",
    "Remember we had framed this back and forth process of the generator ($G$) trying to fool the discriminator ($D$), and the discriminator trying to correctly classify real vs. fake as a minimax game:\n",
    "\n",
    "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "where\n",
    "* $z \\sim p(z)$ are the random noise samples,\n",
    "* $G(z)$ are the generated images using the neural network generator $G$, and\n",
    "* $D$ is the output of the discriminator, specifying the probability of an input being real.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVgCDu6sctSw"
   },
   "source": [
    "---\n",
    "## 2 - GAN Objectives\n",
    "\n",
    "To optimize this minimax game, we will aternate between taking gradient *descent* steps on the objective for $G$, and gradient *ascent* steps on the objective for $D$:\n",
    "1. update the **generator** ($G$) to minimize the probability of the **discriminator making the CORRECT choice**.\n",
    "\n",
    "2. update the **discriminator** ($D$) to maximize the probability of the **discriminator making the CORRECT choice**.\n",
    "\n",
    "While these updates are useful for analysis, they do not perform well in practice. Instead, we will use a different objective when we update the generator: maximize the probability of the **discriminator making the INCORRECT choice**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xvUaF9WctSy"
   },
   "source": [
    "---\n",
    "## 3 - In this Assignment\n",
    "\n",
    "In this assignment, we will alternate the following updates:\n",
    "\n",
    "1. Update the generator ($G$) to maximize the probability of the discriminator making the incorrect choice on generated data:\n",
    "\n",
    "$$\\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "\n",
    "2. Update the discriminator ($D$), to maximize the probability of the discriminator making the correct choice on real and generated data:\n",
    "\n",
    "$$\\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCK04OxLctTA"
   },
   "source": [
    "---\n",
    "## 4 - MNIST GAN\n",
    "\n",
    "GANs are known to depend heavily on the right hyperparameters, and also require many training epochs. To keep things simple, we will test our implementation with the MNIST dataset using three different settings.\n",
    "\n",
    "\n",
    "Here's an example of what your outputs from the 3 different models you're going to train should look like... note that GANs are sometimes finicky, so your outputs might not look exactly like this... this is just meant to be a *rough* guideline of the kind of quality you can expect:\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src='https://image.ibb.co/d3SZsq/gan-outputs-pytorch.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozHDPCYxeUWw"
   },
   "source": [
    "---\n",
    "---\n",
    "# [Part 1] Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12SMnaunBw96"
   },
   "source": [
    "---\n",
    "## 1 - Import Libraries\n",
    "\n",
    "Import requiered libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gyI_HVN-ctSk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import os, shutil\n",
    "import imageio, glob\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChFnN6aNktMf"
   },
   "source": [
    "---\n",
    "## 2 - CPU/GPU Setting\n",
    "This is the optional setting if you're using CPU Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HkAsz5K2ktMh"
   },
   "outputs": [],
   "source": [
    "#This line will ensure the code uses the GPU capabilities of your machine\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "## Uncomment out the following line if you're on a machine with a CPU set up for PyTorch!\n",
    "# dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2yt0aybctTB"
   },
   "source": [
    "---\n",
    "## 3 - MNIST Dataset\n",
    "\n",
    "To simplify our code here, we will use the PyTorch MNIST wrapper, which downloads and loads the MNIST dataset. See the [documentation](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py) for more information about the interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIaMNQRmBdbz"
   },
   "source": [
    "---\n",
    "### a. MNIST Parameters\n",
    "\n",
    "The default parameters will take $5,000$ of the training examples and place them into a validation dataset.\n",
    "\n",
    "The data will be saved into a folder called `MNIST_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DMk4OSViBqLw"
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 50000\n",
    "NUM_VAL   = 5000\n",
    "NOISE_DIM = 96\n",
    "\n",
    "MNIST_DIR = 'MNIST_data'\n",
    "if os.path.exists(MNIST_DIR):\n",
    "    shutil.rmtree(MNIST_DIR)\n",
    "os.mkdir(MNIST_DIR)\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gONoyJdAXUP"
   },
   "source": [
    "---\n",
    "### b. Image Sampler\n",
    "\n",
    "Class below is a sampler routine used to sequentially samples MNIST images  from some offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1WDxga1fAH78"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import sampler\n",
    "\n",
    "class ChunkSampler(sampler.Sampler):\n",
    "    def __init__(self, num_samples, start=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ATT9LmzCFit"
   },
   "source": [
    "---\n",
    "### c. Load MNIST images\n",
    "\n",
    "Load data using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVnqvs6kctTD"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_train = dset.MNIST(MNIST_DIR, train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(mnist_train, batch_size=batch_size,\n",
    "                          sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "mnist_val = dset.MNIST(MNIST_DIR, train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_val = DataLoader(mnist_val, batch_size=batch_size,\n",
    "                        sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdvd_1CFctS1"
   },
   "source": [
    "---\n",
    "## 4 - Helper Functions\n",
    "\n",
    "We provide you with some helper functions needed for this exercise\n",
    "\n",
    "You don't need to do anything in these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okLO72JrwBCR"
   },
   "source": [
    "---\n",
    "### a. Show Images\n",
    "function to display a set of image in a two-dimensional gdrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cibe7q9j-1Xz"
   },
   "outputs": [],
   "source": [
    "def show_images(images):\n",
    "    # images reshape to (batch_size, D)\n",
    "    images  = np.reshape(images, [images.shape[0], -1])\n",
    "    sqrtn   = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs  = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXiWCZTNctTI"
   },
   "source": [
    "Run the code below to show some MNIST data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n4LrFmsctTK"
   },
   "outputs": [],
   "source": [
    "imgs = loader_train.__iter__().next()[0]\n",
    "imgs = imgs.view(batch_size, 784).numpy().squeeze()\n",
    "\n",
    "show_images(imgs[:121])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyCam7Jg-12k"
   },
   "source": [
    "---\n",
    "### b. Preprocess Image\n",
    "\n",
    "`preprocess_img()` function is used to preprocess MNIST dataset images by normalizing them to range of $-1..1$\n",
    "\n",
    "and another `deprocess_img()` function to return the images back to range of $0..1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C0GotRAlctS7"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCnbsppvwD-Q"
   },
   "source": [
    "---\n",
    "### c. Relative Error Function\n",
    "\n",
    "Function to calculate difference between your matrix and our expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KQN7GSth_wMc"
   },
   "outputs": [],
   "source": [
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMMuO-WiA2RQ"
   },
   "source": [
    "---\n",
    "### d. Model Summary\n",
    "\n",
    "Function to display model graph and count its number of parameters\n",
    "\n",
    "Kind of a simple version of keras `.summary()` in PyTorch\n",
    "\n",
    "Actually, you can just use `print()` function to show the model's architecture. But this way, we can also print the number of trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JVzQVK3qA2RY"
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    param_count = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "\n",
    "    base = str(model).split('\\n')\n",
    "    base = base[1:-1]\n",
    "    # base[-1] =\"(-) : Output()\"\n",
    "    summary = []\n",
    "    for layer in base:\n",
    "        new_l = []\n",
    "        layer = layer.replace('in_features=','')\n",
    "        layer = layer.replace('out_features=','')\n",
    "        layer = layer.replace('kernel_size','f')\n",
    "        layer = layer.replace('=(','=[')\n",
    "        layer = layer.replace('), ','], ')\n",
    "        layer = layer.replace('momentum','m')\n",
    "        layer = layer.replace('stride','s')\n",
    "        layer = layer.replace('padding','p')\n",
    "        layer = layer.replace('))','])')\n",
    "        layer = layer.replace(', dilation=1, ceil_mode=False','')\n",
    "        layer = layer.replace('negative_slope','alpha')\n",
    "        layer = layer.replace(', affine=True, track_running_stats=True','')\n",
    "        layer = layer.split(': ')\n",
    "        new_l.append(layer[0].replace(' ',''))\n",
    "        new_l.extend(layer[1].replace('(','\\n(').split('\\n'))\n",
    "        summary.append(new_l)\n",
    "\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(np.prod(param.size()))\n",
    "\n",
    "    i = 0\n",
    "    for s in summary:\n",
    "        if s[1] in ['Linear', 'BatchNorm1d', 'BatchNorm2d', 'Conv2d', 'ConvTranspose2d' ]:\n",
    "            s.append(params[i]+params[i+1])\n",
    "            i+=2\n",
    "\n",
    "    line = \"{: ^4} {:>15}  {:>40} {:>10}\".format(\"ID\", \"Layer (type)\", \"Input-Output Shape\", \"Param #\")\n",
    "    print(line)\n",
    "    print(\"=========================================================================\")\n",
    "    for s in summary:\n",
    "        if(len(s)<4):\n",
    "            s.append('-')\n",
    "        line = \"{: >4} {:>15}  {:>40} {:>10}\".format(*s)\n",
    "        print(line)\n",
    "\n",
    "    # print(model)\n",
    "    print('\\nTotal Parameters: {:,.0f}'.format(param_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ApNwbbGctTa"
   },
   "source": [
    "---\n",
    "### e. Flatten and Unflatten\n",
    "\n",
    "Recall way back in our Task 08 Exercise?\n",
    "\n",
    "There, we provide you with Flatten operation.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "This time we also provide an **Unflatten** operation, which reshape an input vector into a 3-dimensional tensor.\n",
    "\n",
    "You'll it use when implementing the convolutional generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eNb5KKKnctTb"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()                 # read in N, C, H, W\n",
    "        return x.view(N, -1)                  # \"flatten\" the C * H * W values into a single vector per image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDhu-yP2G03p"
   },
   "source": [
    "\n",
    "An Unflatten module receives an input of shape $(N, C^*H^*W)$ and reshapes it to produce an output of shape $(N, C, H, W)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3-I4BnGZGvT_"
   },
   "outputs": [],
   "source": [
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, N=-1, C=128, H=7, W=7):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.N = N\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.N, self.C, self.H, self.W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj_3xhRYGpSm"
   },
   "source": [
    "---\n",
    "### f. Xavier Weight Initializer\n",
    "\n",
    "\n",
    "We also provide a weight initializer (and call it for you) that uses Xavier initialization instead of PyTorch's uniform default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "s3JbrAxiGuT1"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Uit1y7BUYzK"
   },
   "source": [
    "---\n",
    "### g. Generate GIF\n",
    "\n",
    "This function generate a GIF animation from the saved images and display it to your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jOFuzDwAUxVL"
   },
   "outputs": [],
   "source": [
    "def show_gif(base_dir, anim_file):\n",
    "\n",
    "    with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "        filenames = glob.glob(base_dir+'/image*.png')\n",
    "        filenames = sorted(filenames)\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            for i in range(3):\n",
    "              writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "    print('GIF saved as', anim_file)\n",
    "\n",
    "    with open(anim_file,'rb') as f:\n",
    "      display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0m1RBxo6NBW"
   },
   "source": [
    "---\n",
    "## 5 - Load File Checker\n",
    "\n",
    "We also provide a reference matrix to check whether the matrix produced by your implementation matched our expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qWOvhJih6NBa"
   },
   "outputs": [],
   "source": [
    "answers = dict(np.load('gan-checks.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcO47wDG8RsZ"
   },
   "source": [
    "---\n",
    "---\n",
    "# [Part 2] Vanilla GAN\n",
    "\n",
    "For starter, let's build our Vanilla GAN using Normal Neural Network (Linear Layer, not Convolutional Layer)\n",
    "\n",
    "After we build the training function for Vanilla GAN, for the other architecture, it's as simple as replacing the Loss and the Networks (Generator and Discriminator network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmzfc7u2ctTO"
   },
   "source": [
    "---\n",
    "## 1 - Random Noise\n",
    "\n",
    "In a similar way to Variational Autoencoders, a GAN Generator network generates an image from a input random vector, called `seed` or `noise`\n",
    "\n",
    "It is preferable that the seed is uniformly distributed in range of $(-1..1)$\n",
    "\n",
    "However, PyTorch only provides random uniform generator &nbsp;`torch.rand()`&nbsp; function which generate a PyTorch Tensor in range of $(0..1)$\n",
    "\n",
    "So we need to define a function to scale and shift it to the desired range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aorPmaAuI8px"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "\n",
    "Complete the function generate uniform noise from $-1$ to $1$ with shape `[batch_size, dim]`.\n",
    "\n",
    "Hint: use `torch.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ERU1IUcnctTP"
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "\n",
    "    # use torch.rand() to generate random matrix of shape (batch_size, dim)\n",
    "    # then scale (multiply) by 2 and subtract by 1.\n",
    "    noise = ?? \n",
    "\n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXuwExjhctTT"
   },
   "source": [
    "Make sure noise is the correct shape and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XsOUntRctTU"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(135)\n",
    "batch_size = 3\n",
    "dim        = 4\n",
    "\n",
    "z = sample_noise(batch_size, dim)\n",
    "np_z = z.cpu().numpy()\n",
    "print(np_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNFsH0WLctTZ"
   },
   "source": [
    "**Expected Output**:\n",
    "<pre>\n",
    "[[-0.39958  0.53384  0.77969  0.82134]\n",
    " [-0.95613  0.26553  0.84695 -0.49194]\n",
    " [-0.54828  0.73773 -0.33849  0.56049]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3DrTVYMctTj"
   },
   "source": [
    "---\n",
    "## 2 - Discriminator Model\n",
    "Our first step is to design the discriminator. Fill in the architecture as part of the `nn.Sequential` constructor in the function below. All fully connected layers should include bias terms.\n",
    "\n",
    "The architecture is:\n",
    "<pre>\n",
    "    * <b>Flatten</b> layer\n",
    "    * <b>Fully connected</b> layer with input size <font color='blue'><b>784</b></font> and output size <font color='blue'><b>256</b></font>,\n",
    "    * <b>LeakyReLU</b> activation with alpha <font color='blue'><b>0.01</b></font>\n",
    "    * <b>Fully connected</b> layer with input size <font color='blue'><b>256</b></font> and output size <font color='blue'><b>256</b></font>,\n",
    "    * <b>LeakyReLU</b> activation with alpha <font color='blue'><b>0.01</b></font>\n",
    "    * <b>Fully connected</b> layer with input size <font color='blue'><b>256</b></font> and output size <font color='blue'><b>1</b></font>,\n",
    "</pre>\n",
    "\n",
    "\n",
    "Recall Leaky ReLU:\n",
    "  * Leaky ReLU nonlinearity computes&nbsp; $f(x) = \\max(\\alpha x, x)$ &nbsp;for some fixed constant $\\alpha$;\n",
    "  * for this architecture, we will set&nbsp; $\\alpha=0.01$.\n",
    "\n",
    " <br>\n",
    "\n",
    "The output of the discriminator should have shape `[batch_size, 1]`, and contain real numbers corresponding to the scores that each of the `batch_size` inputs is a real image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmC8f-rrIFsA"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "Build and return a PyTorch model implementing the architecture above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3-Q5pNC-ctTl"
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    model = nn.Sequential(\n",
    "\n",
    "        # add Flatten() custom layer\n",
    "        ??,\n",
    "\n",
    "        # add nn.Linear() layer with defined size above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.LeakyRelu() with defined alpha above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.Linear() layer with defined size above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.LeakyRelu() with defined alpha above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.Linear() layer with defined size above,\n",
    "        ??\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dHNuvHoctTn"
   },
   "source": [
    "Test to make sure the number of parameters in the discriminator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwEkkvjqctTp"
   },
   "outputs": [],
   "source": [
    "model = discriminator()\n",
    "\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo9JOxCtctTt"
   },
   "source": [
    "**Expected Result**:\n",
    "\n",
    "<pre>\n",
    " ID     Layer (type)               Input-Output Shape    Param #\n",
    "================================================================\n",
    " (0)         Flatten                               ()          -\n",
    " (1)          Linear            (784, 256, bias=True)     200960\n",
    " (2)       LeakyReLU                     (alpha=0.01)          -\n",
    " (3)          Linear            (256, 256, bias=True)      65792\n",
    " (4)       LeakyReLU                     (alpha=0.01)          -\n",
    " (5)          Linear              (256, 1, bias=True)        257\n",
    "\n",
    "Total Parameters: 267,009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMcEOeroctTt"
   },
   "source": [
    "---\n",
    "## 3 - Generator Model\n",
    "\n",
    "Now we can move on to design the generator network\n",
    "\n",
    "\n",
    "The architecture will be:\n",
    "<pre>\n",
    "    * <b>Flatten</b> layer\n",
    "    * <b>Fully connected</b> layer with input size <font color='blue'><b>noise_dim</b></font> and output size <font color='blue'><b>1024</b></font>,\n",
    "    * <b>ReLU</b> activation\n",
    "    * <b>Fully connected</b> layer with input size <font color='blue'><b>1024</b></font> and output size <font color='blue'><b>1024</b></font>,\n",
    "    * <b>ReLU</b> activation\n",
    "    * <b>Fully connected</b> layer with input size <font color='blue'><b>1024</b></font> and output size <font color='blue'><b>784</b></font>,\n",
    "    * <b>TanH</b> activation\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej2Tc92akD6R"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "Build and return a PyTorch model implementing the architecture above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Z9w7kgrgctTu"
   },
   "outputs": [],
   "source": [
    "def generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"\n",
    "    Build and return a PyTorch model implementing the architecture above.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "\n",
    "        \n",
    "        # add nn.Linear() layer with defined size above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.ReLU() activation\n",
    "        ??,\n",
    "\n",
    "        # add nn.Linear() layer with defined size above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.ReLU() activation\n",
    "        ??,\n",
    "\n",
    "        # add nn.Linear() layer with defined size above,\n",
    "        ??,\n",
    "\n",
    "        # add nn.Tanh() activation\n",
    "        ??\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLjCUaUUctTy"
   },
   "source": [
    "Test to make sure the number of parameters in the generator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93m7E-S1ctTz"
   },
   "outputs": [],
   "source": [
    "model = generator(4)\n",
    "\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwS_vPmuctT3"
   },
   "source": [
    "**Expected Result**:\n",
    "\n",
    "<pre>\n",
    " ID     Layer (type)                 Input-Output Shape    Param #\n",
    "==================================================================\n",
    " (0)          Linear               (4, 1024, bias=True)       5120\n",
    " (1)            ReLU                                 ()          -\n",
    " (2)          Linear            (1024, 1024, bias=True)    1049600\n",
    " (3)            ReLU                                 ()          -\n",
    " (4)          Linear             (1024, 784, bias=True)     803600\n",
    " (5)            Tanh                                 ()          -\n",
    "\n",
    "Total Parameters: 1,858,320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB7RQxe8ctT4"
   },
   "source": [
    "---\n",
    "# [Part 3] GAN Loss\n",
    "\n",
    "Now we will compute the generator and discriminator losses.\n",
    "\n",
    "The generator loss is given by:\n",
    "\n",
    "$$\n",
    "\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]\n",
    "$$\n",
    "\n",
    "and the discriminator loss is:\n",
    "\n",
    "$$\n",
    "\\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note** that these are negated from the equations presented earlier as we will be ***MINIMIZING*** these losses.\n",
    "\n",
    "<br>\n",
    "\n",
    "Instead of computing the expectation of $\\log D(G(z))$, $\\log D(x)$ and $\\log \\left(1-D(G(z))\\right)$, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKe7ZAxNctT5"
   },
   "source": [
    "---\n",
    "## 1 - Binary Cross Entropy\n",
    "\n",
    "To compute the log probability of the true label given the logits output from the discriminator, we need to calculate the Binary Cross Entropy Loss.\n",
    "\n",
    "Given a score $s\\in\\mathbb{R}$ and a label $y\\in\\{0, 1\\}$, the binary cross entropy loss is\n",
    "\n",
    "$$ bce(s, y) = -y * \\log(s) - (1 - y) * \\log(1 - s) $$\n",
    "\n",
    "<br>\n",
    "\n",
    "A naive implementation of this formula can be numerically unstable, so a numerically stable implementation is provided for you below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PvKOijguctT7"
   },
   "outputs": [],
   "source": [
    "def bce_loss(input, target):\n",
    "\n",
    "    neg_abs = - input.abs()\n",
    "\n",
    "    loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUi2ljTHctT9"
   },
   "source": [
    "---\n",
    "## 2 - Data Type conversion\n",
    "\n",
    "You will also need to compute losses corresponding to real or fake data and use the logit arguments to determine their size. This computation will be performed in your selected device (CPU or GPU).\n",
    "\n",
    "If you are using GPU to train the network, you need to cast these labels to the correct data type using the global `dtype` variable, for example:\n",
    "\n",
    "```python\n",
    "      true_labels = torch.ones(size).type(dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AENYMoQpctT_"
   },
   "source": [
    "---\n",
    "## 3 - Discriminator Loss\n",
    "Calculate the total loss of your Discriminator Model in recognizing real images and fake images.\n",
    "\n",
    "The loss formula is as follow:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "or in simple terms:\n",
    "\n",
    "```python\n",
    "      loss = loss_real + loss_fake\n",
    "```\n",
    "\n",
    "where\n",
    "* `loss_real` is the binary cross entropy loss result between `logits_real` and a `ones` matrix, and\n",
    "\n",
    "* `loss_fake` is the binary cross entropy loss result between `logits_fake` and a `zeros` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czXFhry9oeBW"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "\n",
    "Implement the Discriminator Loss as  described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fuUxbbCHctUA"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(logits_real, logits_fake):\n",
    "\n",
    "    # get size of batch data\n",
    "    N = logits_real.size(0)\n",
    "\n",
    "    # create ONES tensor as real-data label\n",
    "    # use torch.ones() with size N, remember to cast the type\n",
    "    # (read instruction/example above)\n",
    "    labels_real = ??\n",
    "\n",
    "    # calculate the loss_real\n",
    "    # call bce_loss() function with input logits_real and labels_real\n",
    "    loss_real = ??\n",
    "\n",
    "    # create ZEROS tensor as fake-data label\n",
    "    # use torch.zeros() with size N, remember to cast the type\n",
    "    # (read instruction/example above)\n",
    "    labels_fake = ??\n",
    "\n",
    "    # calculate the loss_fake\n",
    "    # call bce_loss() function with input logits_fake and labels_fake\n",
    "    loss_fake = ??\n",
    "\n",
    "    # calculate the total discriminator loss\n",
    "    loss = ??\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlNXdOhDctUC"
   },
   "source": [
    "Test your discriminator loss.\n",
    "\n",
    "You should see errors < `1e-7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0N3ysHictUC"
   },
   "outputs": [],
   "source": [
    "d_loss = discriminator_loss(torch.Tensor(answers['logits_real']).type(dtype),\n",
    "                            torch.Tensor(answers['logits_fake']).type(dtype)).cpu().numpy()\n",
    "\n",
    "difference = rel_error(answers['d_loss_true'], d_loss)\n",
    "\n",
    "print(\"Maximum error in d_loss: %g\" % difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8LX-CM7ctUG"
   },
   "source": [
    "**Expected Result**:\n",
    "<pre>\n",
    "Maximum error in d_loss: 2.83811e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQF2v0awctUH"
   },
   "source": [
    "---\n",
    "## 4 - Generator Loss\n",
    "\n",
    "The Generator model is trained to *fool* the Discriminator. It means that we want to maximize the log probability when predicting fake images.\n",
    "\n",
    "Thus, the Generator Loss is calculated using a simple Binary Cross Entropy as follow:\n",
    "\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "\n",
    "the `loss` calculates the binary cross entropy loss result between `logits_fake` and `ones` matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2auHQxvrieG"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "\n",
    "Implement the Generator Loss as  described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Q74gPc5_ctUI"
   },
   "outputs": [],
   "source": [
    "def generator_loss(logits_fake):\n",
    "\n",
    "    # get the number of batch data\n",
    "    N = logits_fake.size(0)\n",
    "\n",
    "    # create ONES tensor as fake-data label\n",
    "    # use torch.ones() with size N, remember to cast the type\n",
    "    labels_fake = ??\n",
    "\n",
    "    # calculate the loss_fake\n",
    "    # call bce_loss() function with input logits_fake and labels_fake\n",
    "    loss = ??\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWuObAAWctUJ"
   },
   "source": [
    "Test your generator and discriminator loss. You should see errors < 1e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWL-jEPkctUK"
   },
   "outputs": [],
   "source": [
    "g_loss = generator_loss(torch.Tensor(answers['logits_fake']).type(dtype)).cpu().numpy()\n",
    "\n",
    "difference = rel_error(answers['g_loss_true'], g_loss)\n",
    "\n",
    "print(\"Maximum error in g_loss: %g\" % difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjx6oE6JctUN"
   },
   "source": [
    "**Expected Result**:\n",
    "<pre>\n",
    "Maximum error in g_loss: 3.4188e-08\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwuN37KxslBO"
   },
   "source": [
    "---\n",
    "# [Part 4] Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0qEBCi5ctUN"
   },
   "source": [
    "\n",
    "## 1 - Adam Optimizer\n",
    "\n",
    "To train our models, we will optimize it using Adam optimizer with a `1e-3` learning rate, beta1=`0.5`, beta2=`0.999`.\n",
    "\n",
    "Note that training a model in PyTorch requires an [Optimizer](https://pytorch.org/docs/stable/optim.html) object to perform the step and weights update.\n",
    "\n",
    "For that, we need two optimizers for each generator and discriminator models. And later, we need another four when we implement LS-GAN and DC-GAN. We set all of them use the same Adam optimizer with the same learning rate and betas.\n",
    "\n",
    "Therefore, rather than defining each of optimizers one-by-one, let's build a function to get the optimizer for any given model. We'll use this to construct optimizers for the generators and discriminators for the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOac3usffMjW"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "\n",
    "Construct and return an Adam optimizer for the model with `learning rate = 1e-3` and `betas = (0.5, 0.999)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LQwnHpxfctUO"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "\n",
    "    # call optim.Adam() function with input model.parameters()\n",
    "    # and defined learning rate and betas\n",
    "    optimizer = ??\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q731E88actUS"
   },
   "source": [
    "---\n",
    "## 2 - Training Loop Function\n",
    "\n",
    "We provide you the main training loop... you won't need to change this function, but reading through and understand it is **highly encouraged**. Notice the order in which the forward and backward passes are performed through both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PMxGHFZxctUT"
   },
   "outputs": [],
   "source": [
    "def train_gan(Discriminator, Generator, base_dir,\n",
    "              show_every=250, batch_size=128, noise_size=96, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train a GAN!\n",
    "\n",
    "    Inputs:\n",
    "    - Discriminator  : Tuple of Discriminator package, containing:\n",
    "      - D_model      : PyTorch models for the discriminator\n",
    "      - D_solver     : torch.optim Optimizers for training\n",
    "      - D_loss       : Functions to use for computing the discriminator loss.\n",
    "\n",
    "    - Generator      : Tuple of Generator package, containing:\n",
    "      - G_model      : PyTorch models for the generator\n",
    "      - G_solver     : torch.optim Optimizers for training\n",
    "      - G_loss       : Functions to use for computing the generator loss.\n",
    "\n",
    "    - base_dir           : Directory to save the generated image example\n",
    "    - show_every         : Show and save samples after every show_every iterations.\n",
    "    - batch_size         : Batch size to use for training.\n",
    "    - noise_size         : Dimension of the noise to use as input to the generator.\n",
    "    - num_epochs         : Number of epochs over the training dataset to use for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack Discriminator\n",
    "    D_model, D_solver, D_loss = Discriminator\n",
    "\n",
    "    # unpack Generator\n",
    "    G_model, G_solver, G_loss = Generator\n",
    "\n",
    "    # parameter setting\n",
    "    iter_count = 0\n",
    "    img_count  = 0\n",
    "    seed_saved = sample_noise(batch_size, noise_size).type(dtype)\n",
    "\n",
    "    # make directory for saving image\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, _ in loader_train:\n",
    "            if len(x) != batch_size:\n",
    "                continue\n",
    "\n",
    "            # reset the gradients\n",
    "            D_solver.zero_grad()\n",
    "\n",
    "            # get real images\n",
    "            real_data   = x.type(dtype)\n",
    "            logits_real = D_model(2* (real_data - 0.5)).type(dtype)\n",
    "\n",
    "            # 1. GENERATOR NETWORK FORWARD\n",
    "            # generate new fake images\n",
    "            g_fake_seed = sample_noise(batch_size, noise_size).type(dtype)\n",
    "            fake_images = G_model(g_fake_seed).detach()\n",
    "            logits_fake = D_model(fake_images.view(batch_size, 1, 28, 28))\n",
    "\n",
    "\n",
    "            # 2. DISCRIMINATOR NETWORK FORWARD\n",
    "            # forward fake and real logits to discriminator\n",
    "            d_total_error = D_loss(logits_real, logits_fake)\n",
    "\n",
    "\n",
    "            # 3. DISCRIMINATOR NETWORK BACKWARD\n",
    "            # backward the error\n",
    "            d_total_error.backward()\n",
    "\n",
    "            # update discriminator weights\n",
    "            D_solver.step()\n",
    "\n",
    "\n",
    "            # 4. GENERATOR NETWORK BACKWARD\n",
    "            # reset the gradients\n",
    "            G_solver.zero_grad()\n",
    "\n",
    "            # generate new fake images\n",
    "            g_fake_seed = sample_noise(batch_size, noise_size).type(dtype)\n",
    "            fake_images = G_model(g_fake_seed)\n",
    "\n",
    "            # get logits fake through discriminator network\n",
    "            gen_logits_fake = D_model(fake_images.view(batch_size, 1, 28, 28))\n",
    "\n",
    "            # calculate generator loss\n",
    "            g_error = G_loss(gen_logits_fake)\n",
    "\n",
    "            # backward the error\n",
    "            g_error.backward()\n",
    "\n",
    "            # update generator weights\n",
    "            G_solver.step()\n",
    "\n",
    "            if (iter_count % show_every == 0):\n",
    "\n",
    "                print('Iter: {}, D loss: {:.4}, G loss:{:.4}'.format(iter_count,d_total_error.item(),g_error.item()))\n",
    "\n",
    "                # show randomly generated images from this iteration\n",
    "                imgs_numpy = fake_images.data.cpu().numpy()\n",
    "                show_images(imgs_numpy[0:16])\n",
    "                plt.show()\n",
    "\n",
    "                # save images generated from the same initial seed\n",
    "                # to visualize the generator progress along training\n",
    "                filename = base_dir+'/image_'+'{:03}'.format(img_count)+'.png'\n",
    "                fake_images = G_model(seed_saved)\n",
    "                imgs_numpy = fake_images.data.cpu().numpy()\n",
    "                show_images(imgs_numpy[0:16])\n",
    "                plt.savefig(filename)\n",
    "                plt.close()\n",
    "                print()\n",
    "                img_count += 1\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "    print('training done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgc-5Bm0k33v"
   },
   "source": [
    "---\n",
    "## 3 - Train Vanilla GAN\n",
    "\n",
    "Now let's try to train a Vanilla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vnoy-EXlctUW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the discriminator\n",
    "D_V = discriminator().type(dtype)\n",
    "\n",
    "# Make the generator\n",
    "G_V = generator().type(dtype)\n",
    "\n",
    "# Use the function you wrote earlier to get optimizers for the Discriminator and the Generator\n",
    "D_solver = get_optimizer(D_V)\n",
    "G_solver = get_optimizer(G_V)\n",
    "\n",
    "# pack the model\n",
    "D = ( D_V, D_solver, discriminator_loss )\n",
    "G = ( G_V, G_solver, generator_loss )\n",
    "\n",
    "# Run it!\n",
    "base_dir = 'v_gan'\n",
    "train_gan(D, G, base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBod32HoctUY"
   },
   "source": [
    "In the iterations in the low 100s you should see black backgrounds, fuzzy shapes as you approach iteration 1000, and decent shapes, about half of which will be sharp and clearly recognizable as we pass 3000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikYqrmMVUj16"
   },
   "source": [
    "---\n",
    "## 4 - Generate GIF\n",
    "\n",
    "Now this just for visualization purposes, we combine the saved image generated from the same initial seed each epoch while training into a GIF animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwLP7vdEXNqg"
   },
   "outputs": [],
   "source": [
    "base_dir = 'v_gan'\n",
    "\n",
    "show_gif(base_dir, 'vanilla_gan.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AhB2wsictUZ"
   },
   "source": [
    "---\n",
    "---\n",
    "# [Part 5] Least Squares GAN\n",
    "We'll now look at [Least Squares GAN](https://arxiv.org/abs/1611.04076), a newer, more stable alernative to the original GAN loss function.\n",
    "\n",
    "For this part, all we have to do is change the loss function and retrain the model.\n",
    "\n",
    "<br>\n",
    "\n",
    "We'll implement equation (9) in the paper, with the generator loss:\n",
    "$$\n",
    "\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "\n",
    "and the discriminator loss:\n",
    "\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFNPOjP_ctUZ"
   },
   "source": [
    "---\n",
    "## 1 - LS-Discriminator Loss\n",
    "\n",
    "In the <font color='red'>**Vanilla GAN**</font> we just implemented, the Discriminator Loss is the total <font color='red'>binary cross entropy loss</font> from discriminator when recognizing real images and fake images. And we train the network to **MINIMIZE** that total loss\n",
    "\n",
    "<br>\n",
    "\n",
    "In <font color='blue'>**LS-GAN**</font>, we change the BCE loss calculation into a simple <font color='blue'>score averaging</font>.\n",
    "\n",
    "The idea is that since the Discriminator is a binary classification, it means that:\n",
    "\n",
    "* we want the `real_score` when predicting **real images** to be **as close to 1 as possible**,\n",
    "\n",
    "* and the `fake_score` when prediction **fake images** to be **as close to 0 as possible**.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now if we ***flip*** the real scores by subtracting it with 1, this equalize our objectives so that now we want both `fake_score` and `real_score-1` to be as close to 0 as possible. And if we square the scores, now the values are always positive, and our objectives all become <font color='blue'>**minimization to 0**</font>.\n",
    "\n",
    "<br>\n",
    "\n",
    "As so we can just simply average the squared scores, and train the network to **MINIMIZE** the average\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$$\n",
    "\\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "or its just simply,\n",
    "\n",
    "```python\n",
    "    loss_real = mean( square( scores_real-1 ) )\n",
    "    loss_fake = mean( square( scores_fake ) )\n",
    "\n",
    "    loss = 0.5 * loss_real + 0.5 * loss_fake\n",
    "```\n",
    "\n",
    "However, instead of computing the expectation, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing.\n",
    "\n",
    "When plugging in for $D(x)$ and $D(G(z))$ use the direct output from the discriminator (`scores_real` and `scores_fake`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3zuwNhNn7Pu"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "\n",
    "Implement the Least Squares GAN Discriminator loss as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Sqpv9waRctUb"
   },
   "outputs": [],
   "source": [
    "def ls_discriminator_loss(scores_real, scores_fake):\n",
    "\n",
    "    # calculate loss_real:\n",
    "    # - first calculate the square of (scores_real-1)\n",
    "    # - use torch.mean with input to average the squared score\n",
    "    loss_real = ??\n",
    "\n",
    "    # calculate loss_fake:\n",
    "    # - use torch.mean() with input square of (scores_fake)\n",
    "    loss_fake = ??\n",
    "\n",
    "    # average the loss_real and loss_fake\n",
    "    loss = ??\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1psf4C9mctUd"
   },
   "source": [
    "Before running a GAN with our new loss function, let's check it:\n",
    "\n",
    "You should see error <`1e-7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeGEoK9sctUe"
   },
   "outputs": [],
   "source": [
    "score_real = torch.Tensor(answers['logits_real']).type(dtype)\n",
    "score_fake = torch.Tensor(answers['logits_fake']).type(dtype)\n",
    "\n",
    "d_loss = ls_discriminator_loss(score_real, score_fake).cpu().numpy()\n",
    "\n",
    "difference = rel_error(answers['d_loss_lsgan_true'], d_loss)\n",
    "\n",
    "print(\"Maximum error in d_loss: %g\" % difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ef2rYhNctUh"
   },
   "source": [
    "**Expected Result**:\n",
    "<pre>\n",
    "Maximum error in g_loss: 1.53171e-08\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXyDMaz3ctUh"
   },
   "source": [
    "---\n",
    "## 2 - LS-Generator Loss\n",
    "\n",
    "Similar to the Vanilla GAN case, the Generator Loss is designed to minimize the correct classification of the discriminator in predicting fake images.\n",
    "\n",
    "In LS-GAN, since the Discriminator is using a simple mean-squared error, then we have to treat the Generator the same way.\n",
    "\n",
    "Thus our objective for Generator Model is to maximize the `fake_score` to be as close to 1 as possible, or flip it to minimizing `fake_score-1` to be as close to 0 as possible\n",
    "\n",
    "And so we have:\n",
    "\n",
    "$$\n",
    "\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "or simply,\n",
    "```python\n",
    "    loss = 1/2 * mean( square( scores_fake-1 ) )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdQ8OufvwkJk"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "\n",
    "Implement the Least Squares GAN Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Pl4n6HxActUi"
   },
   "outputs": [],
   "source": [
    "def ls_generator_loss(scores_fake):\n",
    "\n",
    "    # calculate generator loss,\n",
    "    # first use torch.mean() with input square of (scores_fake-1)\n",
    "    # then divide by 2\n",
    "    loss = ??\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2b9QHumctUk"
   },
   "source": [
    "Before running a GAN with our new loss function, let's check it:\n",
    "\n",
    "You should see error <`1e-7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzrX73H-ctUl"
   },
   "outputs": [],
   "source": [
    "score_real = torch.Tensor(answers['logits_real']).type(dtype)\n",
    "score_fake = torch.Tensor(answers['logits_fake']).type(dtype)\n",
    "\n",
    "d_loss = ls_discriminator_loss(score_real, score_fake).cpu().numpy()\n",
    "g_loss = ls_generator_loss(score_fake).cpu().numpy()\n",
    "\n",
    "g_difference = rel_error(answers['d_loss_lsgan_true'], d_loss)\n",
    "d_difference = rel_error(answers['g_loss_lsgan_true'], g_loss)\n",
    "\n",
    "print(\"Maximum error in d_loss: %g\" % g_difference)\n",
    "print(\"Maximum error in g_loss: %g\" % d_difference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miT18EBFctUn"
   },
   "source": [
    "**Expected Result**:\n",
    "<pre>\n",
    "Maximum error in d_loss: 1.53171e-08\n",
    "Maximum error in g_loss: 2.7837e-09\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIC6iJeNctUn"
   },
   "source": [
    "---\n",
    "## 3 - Train LS-GAN\n",
    "Run the following cell to train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nhn2FlmTctUo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make new discriminator model\n",
    "D_LS = discriminator().type(dtype)\n",
    "\n",
    "# Make new generator model\n",
    "G_LS = generator().type(dtype)\n",
    "\n",
    "# Use the get_optimizer() function to get optimizers for the Discriminator and the Generator\n",
    "D_LS_solver = get_optimizer(D_LS)\n",
    "G_LS_solver = get_optimizer(G_LS)\n",
    "\n",
    "# pack the model into tuple\n",
    "# pack D_LS, D_LS_solver, and ls_discriminator_loss into a single tuple\n",
    "D = ( D_LS, D_LS_solver, ls_discriminator_loss )\n",
    "\n",
    "# pack G_LS, G_LS_solver, and ls_generator_loss into a single tuple\n",
    "G = ( G_LS, G_LS_solver, ls_generator_loss )\n",
    "\n",
    "base_dir = 'ls_gan'\n",
    "\n",
    "# train the networks by calling train_gan() function\n",
    "train_gan(D, G, base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwDClYWK-Hqy"
   },
   "source": [
    "You should see that using LS-GAN, the generated image looks more sharp and clearly recognizable compared to Vanilla GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76uYAfsS3ei1"
   },
   "source": [
    "---\n",
    "## 4 - Generate GIF\n",
    "\n",
    "Now this just for visualization purposes, we combine the saved image generated from the same initial seed each epoch while training into a GIF animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv6E7bc13ei5"
   },
   "outputs": [],
   "source": [
    "base_dir = 'ls_gan'\n",
    "\n",
    "show_gif(base_dir, 'ls_gan.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW9-8WaxctUq"
   },
   "source": [
    "---\n",
    "---\n",
    "# [Part 6] Deeply Convolutional GANs\n",
    "In the first part of the notebook, we implemented an almost direct copy of the original GAN network from Ian Goodfellow.\n",
    "\n",
    "Thus, in this section, we will implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where we use convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LkxLE4CctUr"
   },
   "source": [
    "---\n",
    "## 1 - DC-Discriminator Model\n",
    "\n",
    "We will use a 4 layer ConvNet as our Discriminator Network\n",
    "\n",
    "The architecture is as follow:<pre>\n",
    "    01. <b>Unflatten</b> layer       to reshape input into <font color='blue'><b>(1,28,28)</b></font>\n",
    "    02. <b>Conv2D</b> layer          with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>5x5</b></font> using stride <font color='blue'><b>1</b></font>,\n",
    "    03. <b>LeakyReLU</b> activation  with alpha <font color='blue'><b>0.01</b></font>\n",
    "    04. <b>Max Pool</b> layer        with kernel size of <font color='blue'><b>2x2</b></font>\n",
    "    05. <b>Conv2D</b> layer          with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>5x5</b></font> using stride <font color='blue'><b>1</b></font>,\n",
    "    06. <b>LeakyReLU</b> activation  with alpha <font color='blue'><b>0.01</b></font>\n",
    "    07. <b>Max Pool</b> layer        with kernel size of <font color='blue'><b>2x2</b></font>\n",
    "    08. <b>Flatten</b> layer         to reshape activation into vector\n",
    "    09. <b>Fully Connected</b>       with input size <font color='blue'><b>4\\*4\\*64</b></font> and output size also <font color='blue'><b>4\\*4\\*64</b></font>,\n",
    "    10. <b>LeakyReLU</b> activation  with alpha <font color='blue'><b>0.01</b></font>\n",
    "    11. <b>Fully Connected</b>       with input size <font color='blue'><b>4\\*4\\*64</b></font> and output size <font color='blue'><b>1</b></font>,\n",
    "</pre>\n",
    "\n",
    "---\n",
    "Read the [Documentation](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-k26867Im5G"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "Build and return a PyTorch model implementing the architecture above.\n",
    "\n",
    "Note that:\n",
    "\n",
    "The [.Conv2d()](https://pytorch.org/docs/stable/nn.html#conv2d) layer from .nn module receives the required inputs as follows:\n",
    "<pre>\n",
    "* in_channel  : input channel dimension\n",
    "* out_channel : output channel dimension (number of filter)\n",
    "* kernel_size : either integer or tuple\n",
    "* stride      : optional argument, default=1</pre>\n",
    "\n",
    "while the [.MaxPool2d()](https://pytorch.org/docs/stable/nn.html#maxpool2d) layer only receives the required inputs as follows:\n",
    "<pre>\n",
    "* kernel_size : either integer or tuple\n",
    "* stride      : optional argument, default equal to kernel_size</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rafpCGIcctUs"
   },
   "outputs": [],
   "source": [
    "def dc_discriminator():\n",
    "\n",
    "    return nn.Sequential(\n",
    "        # 01. call Unflatten() function with input N=batch_size, C=1, W=28, and H=28\n",
    "        Unflatten(N=batch_size, C=1, W=28, H=28),\n",
    "\n",
    "        # 02. add Conv2d() from .nn module with 32 Filters, kernel size 5, Stride 1\n",
    "        # (from in_channels = 1)\n",
    "        ??,\n",
    "\n",
    "        # 03. add Leaky ReLU from .nn module with alpha=0.01\n",
    "        ??,\n",
    "\n",
    "        # 04. add MaxPool2d() from .nn module with kernel size 2, Stride 2\n",
    "        ??,\n",
    "\n",
    "        # 05. add Conv2d() from .nn module with 64 Filters, kernel size 5, Stride 1\n",
    "        # (from in_channels = 32)\n",
    "        ??,\n",
    "\n",
    "        # 06. add Leaky ReLU from .nn module with alpha=0.01\n",
    "        ??,\n",
    "\n",
    "        # 07. add MaxPool2d() from .nn module with kernel size 2x2, Stride 2\n",
    "        ??,\n",
    "\n",
    "        # 08. add Flatten() function\n",
    "        ??,\n",
    "\n",
    "        # 09. add Linear layer from .nn module\n",
    "        # with both input and output size 4*4*64\n",
    "        ??,\n",
    "\n",
    "        # 10. add Leaky ReLU from .nn module with alpha=0.01\n",
    "        ??,\n",
    "\n",
    "        # 11. add Linear layer from .nn module\n",
    "        # with input 4*4*64 and output 1\n",
    "        ??\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMqnJDpuO5lb"
   },
   "source": [
    "Test to make sure the number of parameters in the discriminator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bqcnFp-O5li"
   },
   "outputs": [],
   "source": [
    "model = dc_discriminator()\n",
    "\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIQOBWMO5ls"
   },
   "source": [
    "**Expected Result**:\n",
    "\n",
    "<pre>\n",
    " ID     Layer (type)                      Input-Output Shape    Param #\n",
    "=======================================================================\n",
    " (0)       Unflatten                                      ()          -\n",
    " (1)          Conv2d             (1, 32, f=[5, 5], s=[1, 1])        832\n",
    " (2)       LeakyReLU                            (alpha=0.01)          -\n",
    " (3)       MaxPool2d                         (f=2, s=2, p=0)          -\n",
    " (4)          Conv2d            (32, 64, f=[5, 5], s=[1, 1])      51264\n",
    " (5)       LeakyReLU                            (alpha=0.01)          -\n",
    " (6)       MaxPool2d                         (f=2, s=2, p=0)          -\n",
    " (7)         Flatten                                      ()          -\n",
    " (8)          Linear                 (1024, 1024, bias=True)    1049600\n",
    " (9)       LeakyReLU                            (alpha=0.01)          -\n",
    "(10)          Linear                    (1024, 1, bias=True)       1025\n",
    "\n",
    "Total Parameters: 1,102,721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hKaYDR9PM3x"
   },
   "source": [
    "Now try to feed an input image and check the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzkHy56zPA_1"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data = next(enumerate(loader_train))[-1][0].type(dtype)\n",
    "print('Data shape   :', data.shape)\n",
    "\n",
    "b   = dc_discriminator().type(dtype)\n",
    "out = b(data)\n",
    "print('Output shape :', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tN-BpBfctUu"
   },
   "source": [
    "**Expected Result**:\n",
    "<pre>\n",
    "Data shape   : torch.Size([128, 1, 28, 28])\n",
    "Output shape : torch.Size([128, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyEMzdm5ctUz"
   },
   "source": [
    "---\n",
    "## 2 - DC-Generator Model\n",
    "For the generator, we will use the exact architecture from the [InfoGAN paper](https://arxiv.org/pdf/1606.03657.pdf).\n",
    "\n",
    "The architecture is as follow:<pre>\n",
    "    01. <b>Fully connected</b>  with input size <font color='blue'><b>noise_dim</b></font> and output size <font color='blue'><b>1024</b></font>,\n",
    "    02. <b>ReLU</b> activation\n",
    "    03. <b>BatchNorm1D</b>      with input size <font color='blue'><b>1024</b></font>\n",
    "    04. <b>Fully connected</b>  with input size <font color='blue'><b>1024</b></font> and output size <font color='blue'><b>128\\*7\\*7</b></font>,\n",
    "    05. <b>ReLU</b> activation\n",
    "    06. <b>BatchNorm1D</b>      with input size <font color='blue'><b>128\\*7\\*7</b></font>\n",
    "    07. <b>Unflatten</b> layer  to reshape activation into <font color='blue'><b>(128,7,7)</b></font>\n",
    "    08. <b>Conv2DTranspose</b>  with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>4x4</b></font> using stride <font color='blue'><b>2</b></font>, and <font color='blue'><b>padding 1</b></font>\n",
    "    09. <b>ReLU</b> activation\n",
    "    10. <b>BatchNorm2D</b>      with input size <font color='blue'><b>64</b></font>\n",
    "    11. <b>Conv2DTranspose</b>  with <font color='blue'><b>1</b></font> filters of <font color='blue'><b>4x4</b></font> using stride <font color='blue'><b>2</b></font>, and <font color='blue'><b>padding 1</b></font>\n",
    "    12. <b>TanH</b> activation\n",
    "    13. <b>Flatten</b> layer    to reshape activation into <font color='blue'><b>784</b></font> vector    \n",
    "</pre>\n",
    "\n",
    "---\n",
    "Read the [Documentation](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P00D4tTmhu66"
   },
   "source": [
    "<font size=3><b> Something Weird? </b></font>\n",
    "\n",
    "Note that the architecture reshape back it's already made output image of $(1,28,28)$ into a vector if $(784,)$\n",
    "\n",
    "<br>\n",
    "\n",
    "Well, we need to reshape it back since we made our training function to receive and save images from a vector-shaped output from the previous Vanilla GAN\n",
    "\n",
    "Since we don't want to change the training function just for this DC-GAN, it's easier to design the generator model to just return a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVVu478gcPPo"
   },
   "source": [
    "---\n",
    "#### <font color='red'>**YOUR CODE HERE:** </font>\n",
    "Build and return a PyTorch model implementing the architecture above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "HjEFnznFctUz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dc_generator(noise_dim=NOISE_DIM):\n",
    "\n",
    "    return nn.Sequential(\n",
    "\n",
    "        # 01. add Linear() with input noise_dim and output 1024\n",
    "        ??,\n",
    "\n",
    "        # 02. add ReLU() from .nn module\n",
    "        ??,\n",
    "\n",
    "        # 03. add BatchNorm1d() with input 1024\n",
    "        ??,\n",
    "\n",
    "        # 04. add Linear() with input 1024 and output 7*7*128\n",
    "        ??,\n",
    "\n",
    "        # 05. add ReLU() from .nn module\n",
    "        ??,\n",
    "\n",
    "        # 06. add BatchNorm1d() with input 7*7*128\n",
    "        ??,\n",
    "\n",
    "        # 07. add Unflatten() with input N=batch_size, C=128, W=7, and H=7\n",
    "        ??,\n",
    "\n",
    "        # 08. add ConvTranspose2d() with in_channels = 128, 64 Filters,\n",
    "        # kernel size 4, stride 2, and padding 1\n",
    "        ??,\n",
    "\n",
    "        # 09. add ReLU() from .nn module\n",
    "        ??,\n",
    "\n",
    "        # 10. add BatchNorm2d() with input 64\n",
    "        ??,\n",
    "\n",
    "        # 11. add ConvTranspose2d() with input channel = 64, 1 Filter,\n",
    "        # kernel size 4, stride 2, and padding 1\n",
    "        ??,\n",
    "\n",
    "        # 12. add Tanh() from .nn module\n",
    "        ??,\n",
    "\n",
    "        # 13. add back Flatten()\n",
    "        ??\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulAX1YIUkJfZ"
   },
   "source": [
    "Test to make sure the number of parameters in the discriminator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiGZ8aDkkJff"
   },
   "outputs": [],
   "source": [
    "model = dc_generator(4)\n",
    "\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B794kUoFkJfr"
   },
   "source": [
    "**Expected Result**:\n",
    "\n",
    "<pre>\n",
    " ID     Layer (type)                        Input-Output Shape    Param #\n",
    "=========================================================================\n",
    " (0)          Linear                      (4, 1024, bias=True)       5120\n",
    " (1)            ReLU                                        ()          -\n",
    " (2)     BatchNorm1d                  (1024, eps=1e-05, m=0.1)       2048\n",
    " (3)          Linear                   (1024, 6272, bias=True)    6428800\n",
    " (4)            ReLU                                        ()          -\n",
    " (5)     BatchNorm1d                  (6272, eps=1e-05, m=0.1)      12544\n",
    " (6)       Unflatten                                        ()          -\n",
    " (7) ConvTranspose2d   (128, 64, f=[4, 4], s=[2, 2], p=[1, 1])     131136\n",
    " (8)            ReLU                                        ()          -\n",
    " (9)     BatchNorm2d                    (64, eps=1e-05, m=0.1)        128\n",
    "(10) ConvTranspose2d     (64, 1, f=[4, 4], s=[2, 2], p=[1, 1])       1025\n",
    "(11)            Tanh                                        ()          -\n",
    "(12)         Flatten                                        ()          -\n",
    "\n",
    "Total Parameters: 6,580,801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKAUZL-KkJfs"
   },
   "source": [
    "Now try to feed an input image and check the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxoTuKu7kAXM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "test_g_gan = dc_generator().type(dtype)\n",
    "test_g_gan.apply(initialize_weights)\n",
    "\n",
    "fake_seed   = torch.randn(batch_size, NOISE_DIM).type(dtype)\n",
    "fake_images = test_g_gan.forward(fake_seed)\n",
    "\n",
    "print('Output shape :',fake_images.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lgg2UAR7ctU2"
   },
   "source": [
    "**Expected Result**:\n",
    "<pre>\n",
    "Output shape : torch.Size([128, 784])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls03xv6d8Oz4"
   },
   "source": [
    "---\n",
    "## 3 - Train DC-GAN\n",
    "Run the following cell to train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRBmShhf8O0L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make new discriminator model\n",
    "D_DC = dc_discriminator().type(dtype)\n",
    "\n",
    "# Make new generator model\n",
    "G_DC = dc_generator().type(dtype)\n",
    "\n",
    "# apply weight initialization\n",
    "G_DC.apply(initialize_weights)\n",
    "D_DC.apply(initialize_weights)\n",
    "\n",
    "\n",
    "# Use the get_optimizer() function to get optimizers for the Discriminator and the Generator\n",
    "D_DC_solver = get_optimizer(D_DC)\n",
    "G_DC_solver = get_optimizer(G_DC)\n",
    "\n",
    "# pack the model into tuple\n",
    "# pack D_dc, D_dc_solver, and discriminator_loss into a single tuple\n",
    "D = ( D_DC, D_DC_solver, discriminator_loss )\n",
    "\n",
    "# pack G_dc, G_dc_solver, and generator_loss into a single tuple\n",
    "G = ( G_DC, G_DC_solver, generator_loss )\n",
    "\n",
    "base_dir = 'dc_gan'\n",
    "\n",
    "# train the networks by calling train_gan() function\n",
    "train_gan(D, G, base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTkpwOD58O0S"
   },
   "source": [
    "You should see that now using DC-GAN, from the start, the generated image looks much more clean and natural compared to using Linear fully connected GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBAjP-ue8O0V"
   },
   "source": [
    "---\n",
    "## 4 - Generate GIF\n",
    "\n",
    "Now this just for visualization purposes, we combine the saved image generated from the same initial seed each epoch while training into a GIF animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CUFPbUb8O0W"
   },
   "outputs": [],
   "source": [
    "base_dir = 'dc_gan'\n",
    "\n",
    "show_gif(base_dir, 'dc_gan.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoGECWnwctU-"
   },
   "source": [
    "---\n",
    "# [Part 7] Compare generated images\n",
    "\n",
    "Let's see the result of our GAN with the same sample seed start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXxgRLTmctU_"
   },
   "source": [
    "---\n",
    "## 1 - Generate random noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPrfTFL4ctVA"
   },
   "outputs": [],
   "source": [
    "fake_seed = sample_noise(128, 96).type(dtype)\n",
    "\n",
    "print(fake_seed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqdouV-LctVC"
   },
   "source": [
    "Example of seed of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_aHbw-qctVC"
   },
   "outputs": [],
   "source": [
    "print(fake_seed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imMJbbbHctVF"
   },
   "source": [
    "---\n",
    "## 2 - Vanilla GAN Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiWSU7jvctVG"
   },
   "outputs": [],
   "source": [
    "fake_images = G_V(fake_seed)\n",
    "\n",
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-b_n1v99iFq"
   },
   "source": [
    "View the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYbWFbXgctVI"
   },
   "outputs": [],
   "source": [
    "image_to_view = 20\n",
    "imgs_numpy = fake_images.data.cpu().numpy()\n",
    "show_images(imgs_numpy[0:image_to_view])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEuZ-XBU-Muy"
   },
   "source": [
    "Lets compare the results with the real MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8pOUYJp-Mu4"
   },
   "outputs": [],
   "source": [
    "imgs = loader_train.__iter__().next()[0]\n",
    "imgs = imgs.view(batch_size, 784).numpy().squeeze()\n",
    "\n",
    "combined = np.vstack([imgs_numpy[:50,:], imgs[:50,:]])\n",
    "\n",
    "show_images(combined)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMvpMkYQctVK"
   },
   "source": [
    "---\n",
    "## 3 - LS-GAN Generated Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VZ_7D0_ctVK"
   },
   "outputs": [],
   "source": [
    "fake_images = G_LS(fake_seed)\n",
    "\n",
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jMZPDEI9yT-"
   },
   "source": [
    "View the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbnmH9nVctVN"
   },
   "outputs": [],
   "source": [
    "image_to_view = 20\n",
    "imgs_numpy = fake_images.data.cpu().numpy()\n",
    "show_images(imgs_numpy[0:image_to_view])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSs2ObJD-THU"
   },
   "source": [
    "Lets compare the results with the real MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foBcItTj-THv"
   },
   "outputs": [],
   "source": [
    "imgs = loader_train.__iter__().next()[0]\n",
    "imgs = imgs.view(batch_size, 784).numpy().squeeze()\n",
    "\n",
    "combined = np.vstack([imgs_numpy[:50,:], imgs[:50,:]])\n",
    "\n",
    "show_images(combined)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5VrHdb1ctVP"
   },
   "source": [
    "---\n",
    "## 4 - DC-GAN Generated Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rscjlgg3ctVQ"
   },
   "outputs": [],
   "source": [
    "fake_images = G_DC(fake_seed)\n",
    "\n",
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUIF9Zx39ysh"
   },
   "source": [
    "View the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V7y5ygqctVR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_to_view = 25\n",
    "imgs_numpy = fake_images.data.cpu().numpy()\n",
    "show_images(imgs_numpy[0:image_to_view])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnqaVmxFIH45"
   },
   "source": [
    "Looks great isn't it?\n",
    "\n",
    "Lets compare the results with the real MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHgfHD9vIN-z"
   },
   "outputs": [],
   "source": [
    "imgs = loader_train.__iter__().next()[0]\n",
    "imgs = imgs.view(batch_size, 784).numpy().squeeze()\n",
    "\n",
    "combined = np.vstack([imgs_numpy[:50,:], imgs[:50,:]])\n",
    "\n",
    "show_images(combined)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWG28k59IRR3"
   },
   "source": [
    "Can you tell which are the original images and which are the fake ones?\n",
    "\n",
    "Can you tell the difference?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CV2020 - 14 - Generative Adversarial Network (PyTorch).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
