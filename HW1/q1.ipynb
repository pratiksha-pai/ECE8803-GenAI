{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black and White Image Generation with a Probabilistic Model\n",
    "\n",
    "In this notebook, we'll develop a probabilistic model that generates 3x3 black and white images. The probability of generating each image will be proportional to the number of black pixels in the image.\n",
    "\n",
    "We will use the Categorical distribution from PyTorch to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate All Possible 3x3 Images\n",
    "First, we'll generate all possible 3x3 black and white images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels = 9  # 3x3 image\n",
    "all_images = [np.array([int(x) for x in format(i, '0' + str(total_pixels) + 'b')]).reshape(3, 3) \n",
    "              for i in range(2 ** total_pixels)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Probabilities\n",
    "Now, we'll calculate probabilities that are proportional to the number of black pixels in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [np.sum(image) for image in all_images]\n",
    "probs = np.array(probs) / np.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Categorical Distribution\n",
    "We'll use PyTorch's Categorical distribution to sample images based on the calculated probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dist = dist.Categorical(torch.tensor(probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and Visualize Images\n",
    "Finally, we'll draw five samples and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = [all_images[categorical_dist.sample().item()] for _ in range(5)]\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, sample in enumerate(samples):\n",
    "    axes[i].imshow(sample, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks performed until this point \n",
    "1. Imports the required libraries.\n",
    "2. Generates all possible 3x3 black and white images.\n",
    "3. Calculates probabilities proportional to the number of black pixels in each image.\n",
    "4. Creates a Categorical distribution using these probabilities.\n",
    "5. Draws five samples from this distribution and visualizes them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Data Generation Rule with MLP\n",
    "\n",
    "We no longer know the data generation rule and want to learn it from 200 unique data points. We'll consider the probability of generating each training image as the label and train a Multilayer Perceptron (MLP) to develop a new probabilistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 200 Unique Images\n",
    "Let's first sample 200 unique images from our original Categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_samples_index = torch.unique(dist.Categorical(torch.tensor(probs)).sample((512,)), sorted=True)[:200]\n",
    "unique_probs = probs[unique_samples_index]\n",
    "unique_samples = [all_images[i] for i in unique_samples_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_222527/3730610889.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  flattened_samples = torch.tensor([img.flatten() for img in unique_samples]).type(torch.FloatTensor)\n"
     ]
    }
   ],
   "source": [
    "flattened_samples = torch.tensor([img.flatten() for img in unique_samples]).type(torch.FloatTensor)\n",
    "labels = torch.tensor(unique_probs).type(torch.FloatTensor)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(flattened_samples, labels, test_size=0.2, random_state=42)\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(input_size, hidden_size),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "input_size = 9  # 3x3 image\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a New Probabilistic Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Develop a New Probabilistic Model\n",
    "with torch.no_grad():\n",
    "    all_flattened_images = torch.tensor([img.flatten() for img in all_images]).type(torch.FloatTensor)\n",
    "    predicted_probs = model(all_flattened_images).flatten()  # Flatten the tensor to one dimension\n",
    "    normalized_predicted_probs = torch.nn.functional.softmax(predicted_probs, dim=0)  # No need for .numpy()\n",
    "\n",
    "# Create new Categorical distribution with normalized predicted probabilities\n",
    "new_categorical_dist = dist.Categorical(normalized_predicted_probs)\n",
    "\n",
    "# Sample and visualize new images\n",
    "new_samples = [all_images[new_categorical_dist.sample().item()] for _ in range(5)]\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, sample in enumerate(new_samples):\n",
    "    axes[i].imshow(sample, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Data Generation Rule for 28x28 Images with MLP\n",
    "\n",
    "This notebook aims to extend the previous approach to 28x28 dimensions. We'll draw 200 random samples, train an MLP, and discuss why this strategy does not scale well for higher dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Import Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Parameters\n",
    "width, height = 28, 28\n",
    "total_pixels = width * height\n",
    "input_size = total_pixels\n",
    "hidden_size = 64\n",
    "output_size = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw 200 Random Samples\n",
    "Here, we'll employ a categorical distribution to generate 200 random 28x28 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_size = (28, 28)\n",
    "\n",
    "# Generate probabilities proportional to the number of black pixels\n",
    "total = sum(range(1, (28*28) + 1))\n",
    "probabilities = [i / total for i in range(28 * 28)]\n",
    "probabilities = torch.tensor(probabilities)\n",
    "\n",
    "categorical_dist = dist.Categorical(probabilities)\n",
    "\n",
    "\n",
    "# Draw 5 samples from the distribution instead of 200\n",
    "num_samples = 5\n",
    "samples = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    num_black_pixels = categorical_dist.sample()\n",
    "    image = torch.zeros(*image_size)\n",
    "    black_indices = torch.randperm(image_size[0] * image_size[1])[:num_black_pixels]\n",
    "    image.view(-1)[black_indices] = 1\n",
    "    samples.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "for i, sample in enumerate(samples):\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(sample, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Sample {i + 1}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(input_size, hidden_size),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 28, 28])\n",
      "torch.Size([200])\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate some mock data (replace this with your actual data)\n",
    "N = 200  # Number of samples\n",
    "X = torch.randn(N, 28*28)  # 28x28 flattened to 784\n",
    "y = torch.randn(N, 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize MLP, Loss, and Optimizer\n",
    "input_size = 28 * 28\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # if epoch % 100 == 0:\n",
    "    #     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "total_pixels = 28*28\n",
    "all_images = [np.array([int(x) for x in format(i, '0' + str(total_pixels) + 'b')]).reshape(28, 28) \n",
    "              for i in range(2 ** total_pixels)]\n",
    "print(all_images[0:10])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mlp_samples = []\n",
    "    all_probabilities = []\n",
    "    num_pixels = []\n",
    "    for image in all_images:\n",
    "        all_probabilities.append(float(model(torch.FloatTensor(image).view(-1, input_size))))\n",
    "        num_pixels.append(np.count_nonzero(image == 0))\n",
    "        if np.count_nonzero(image == 0) == 0:\n",
    "            print(float(model(torch.FloatTensor(image).view(-1, input_size))))\n",
    "    sum_probabilities = {}\n",
    "    count_occurrences = {}\n",
    "    for num_pixel, probability in zip(num_pixels, all_probabilities):\n",
    "        if num_pixel in sum_probabilities:\n",
    "            sum_probabilities[num_pixel] += probability\n",
    "            count_occurrences[num_pixel] += 1\n",
    "        else:\n",
    "            sum_probabilities[num_pixel] = probability\n",
    "            count_occurrences[num_pixel] = 1\n",
    "\n",
    "# Calculate the average probabilities for each unique value\n",
    "average_probabilities = {num_pixel: sum_probabilities[num_pixel] / count_occurrences[num_pixel] for num_pixel in sum_probabilities}\n",
    "print(average_probabilities)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges in Scaling the MLP for High-Dimensional Generative Modeling\n",
    "\n",
    "##### 1. Memory Limitations\n",
    "Generating all possible $2^{28 \\times 28}$ image combinations is computationally infeasible and kernel terminates due to out-of-memory errors.\n",
    "\n",
    "##### 2. Model Complexity\n",
    "The MLP architecture is too simplistic to capture the intricate, high-dimensional data distribution. The model fails to capture the underlying data distribution effectively.\n",
    "\n",
    "##### 3. Inefficiency of Rejection Sampling\n",
    "The model's predictions are not aligning well with the true data distribution. A 20-minute run of a rejection sampling algorithm failed to produce any samples that met the acceptance criteria.\n",
    "\n",
    "These limitations highlight the challenges of using a rudimentary MLP and naive sampling methods for high-dimensional generative tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
